{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning into a dataframe because easiest way I can find online to pretty format this \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from pickle import load\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../exploratory_table.pickle\", \"rb\") as handle:\n",
    "    table = load(handle)\n",
    "\n",
    "df = pd.DataFrame.from_dict(table, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# GloVe Gigaword 100 dimensions\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m word_vectors \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mglove-wiki-gigaword-100\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/gensim/downloader.py:503\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, return_path)\u001b[0m\n\u001b[1;32m    501\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39minsert(\u001b[39m0\u001b[39m, BASE_DIR)\n\u001b[1;32m    502\u001b[0m module \u001b[39m=\u001b[39m \u001b[39m__import__\u001b[39m(name)\n\u001b[0;32m--> 503\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39;49mload_data()\n",
      "File \u001b[0;32m~/gensim-data/glove-wiki-gigaword-100/__init__.py:8\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_data\u001b[39m():\n\u001b[1;32m      7\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(base_dir, \u001b[39m'\u001b[39m\u001b[39mglove-wiki-gigaword-100\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mglove-wiki-gigaword-100.gz\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     model \u001b[39m=\u001b[39m KeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(path)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/gensim/models/keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m         )\n\u001b[1;32m   2068\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2069\u001b[0m         _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[1;32m   2070\u001b[0m \u001b[39mif\u001b[39;00m kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(kv):\n\u001b[1;32m   2071\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2073\u001b[0m         kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(kv),\n\u001b[1;32m   2074\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1974\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[39mif\u001b[39;00m line \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1973\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1974\u001b[0m word, weights \u001b[39m=\u001b[39m _word2vec_line_to_vector(line, datatype, unicode_errors, encoding)\n\u001b[1;32m   1975\u001b[0m _add_word_to_kv(kv, counts, word, weights, vocab_size)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1980\u001b[0m, in \u001b[0;36m_word2vec_line_to_vector\u001b[0;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_word2vec_line_to_vector\u001b[39m(line, datatype, unicode_errors, encoding):\n\u001b[1;32m   1979\u001b[0m     parts \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(line\u001b[39m.\u001b[39mrstrip(), encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39municode_errors)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1980\u001b[0m     word, weights \u001b[39m=\u001b[39m parts[\u001b[39m0\u001b[39m], [datatype(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m parts[\u001b[39m1\u001b[39m:]]\n\u001b[1;32m   1981\u001b[0m     \u001b[39mreturn\u001b[39;00m word, weights\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/gensim/models/keyedvectors.py:1980\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_word2vec_line_to_vector\u001b[39m(line, datatype, unicode_errors, encoding):\n\u001b[1;32m   1979\u001b[0m     parts \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(line\u001b[39m.\u001b[39mrstrip(), encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39municode_errors)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1980\u001b[0m     word, weights \u001b[39m=\u001b[39m parts[\u001b[39m0\u001b[39m], [datatype(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m parts[\u001b[39m1\u001b[39m:]]\n\u001b[1;32m   1981\u001b[0m     \u001b[39mreturn\u001b[39;00m word, weights\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GloVe Gigaword 100 dimensions\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = table.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_tokens = set()\n",
    "for token in tokens:\n",
    "    try:\n",
    "        word_vectors.get_vector(token)\n",
    "    except KeyError:\n",
    "        oov_tokens.add(token)\n",
    "len(oov_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mPPMI\u001b[39m\u001b[39m\"\u001b[39m], ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m filtered_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mloc[(df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39misin(oov_tokens))]\n\u001b[1;32m      5\u001b[0m \u001b[39m# We can set a cutoff for a good PPMI to explore \u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# filtered_df = filtered_df.loc[df[\"PPMI\"] < 0.7]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# filtered_df = filtered_df.loc[(df[\"M count\"] > 20) & (df[\"F count\"] > 20)]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df = df.sort_values(by=[\"PPMI\"], ascending=False)\n",
    "\n",
    "filtered_df = df.loc[(df.index.isin(oov_tokens))]\n",
    "\n",
    "# We can set a cutoff for a good PPMI to explore \n",
    "# filtered_df = filtered_df.loc[df[\"PPMI\"] < 0.7]\n",
    "# filtered_df = filtered_df.loc[(df[\"M count\"] > 20) & (df[\"F count\"] > 20)]\n",
    "\n",
    "filtered_df[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(\"oov_tokens.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
