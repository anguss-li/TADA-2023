{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pickle import load, dump, HIGHEST_PROTOCOL\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "BOOTSTRAP_SIZE = 1000\n",
    "RANDOM_SEED = 375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"alc_stepone.pickle\", \"rb\") as handle:\n",
    "    alc_stepone = load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range = alc_stepone[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ols(X: np.array, Y: np.array) -> np.array:\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    # LinearRegression does not impose penalty by default\n",
    "    full_sample_out = LinearRegression(fit_intercept=False).fit(X, Y)\n",
    "    beta_coefficients = full_sample_out.coef_\n",
    "    normed_betas = np.linalg.norm(beta_coefficients, axis=0)\n",
    "    return normed_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test1(\n",
    "    token: Dict[str, np.array], rng: np.random.Generator, confidence: float = 90\n",
    ") -> Dict[str, np.array]:\n",
    "    def sample():\n",
    "        Y_permuted = rng.permutation(Y)\n",
    "        return run_ols(X, Y_permuted)\n",
    "\n",
    "    X, Y = token[\"X\"], token[\"Y\"]\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    normed_betas = [sample() for _ in range(BOOTSTRAP_SIZE)]\n",
    "    print(\"done samples\")\n",
    "\n",
    "    # Getting confidence interval\n",
    "    offset = (100 - confidence) / 2\n",
    "    ci_normed_betas = np.percentile(normed_betas, [offset, confidence + offset], axis=0)\n",
    "\n",
    "    # Conducting permutation test: p-value is %values \"more extreme\" than ground_truth\n",
    "    ground_truth = run_ols(X, Y)\n",
    "    empirical_pvalue = np.apply_along_axis(\n",
    "        lambda a: sum(1 if x > ground_truth else 0 for x in a) / len(a),\n",
    "        axis=0,\n",
    "        arr=normed_betas,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"token\": token[\"token\"],\n",
    "        \"ground truth\": ground_truth,\n",
    "        \"CI\": ci_normed_betas,\n",
    "        \"p-value\": empirical_pvalue,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Using backend LokyBackend with 24 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=24)]: Done  14 out of  30 | elapsed:   13.7s remaining:   15.7s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n",
      "done samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m child_seeds \u001b[39m=\u001b[39m ss\u001b[39m.\u001b[39mspawn(\u001b[39mlen\u001b[39m(test_range))\n\u001b[1;32m      3\u001b[0m streams \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mdefault_rng(seed) \u001b[39mfor\u001b[39;00m seed \u001b[39min\u001b[39;00m child_seeds]\n\u001b[0;32m----> 5\u001b[0m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)(\n\u001b[1;32m      6\u001b[0m     delayed(test1)(token, rng) \u001b[39mfor\u001b[39;49;00m token, rng \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(test_range, streams)\n\u001b[1;32m      7\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/concurrent/futures/_base.py:439\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 439\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    442\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp-research/lib/python3.8/threading.py:302\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 302\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    303\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ss = np.random.SeedSequence(RANDOM_SEED)\n",
    "child_seeds = ss.spawn(len(test_range))\n",
    "streams = [np.random.default_rng(seed) for seed in child_seeds]\n",
    "\n",
    "Parallel(n_jobs=24, verbose=1)(\n",
    "    delayed(test1)(token, rng) for token, rng in zip(test_range, streams)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
